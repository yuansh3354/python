{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "political-jungle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:58:12.525983Z",
     "start_time": "2021-04-14T03:58:11.707506Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize, rescale, rotate\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modular-manor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T04:20:46.038865Z",
     "start_time": "2021-04-14T03:59:48.757321Z"
    },
    "code_folding": [
     21,
     38,
     71,
     196,
     209,
     253,
     269,
     422,
     439,
     452,
     463,
     472,
     710
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading train images...\n",
      "preprocessing train volumes...\n",
      "cropping train volumes...\n",
      "padding train volumes...\n",
      "resizing train volumes...\n",
      "normalizing train volumes...\n",
      "done creating train dataset\n",
      "reading validation images...\n",
      "preprocessing validation volumes...\n",
      "cropping validation volumes...\n",
      "padding validation volumes...\n",
      "resizing validation volumes...\n",
      "normalizing validation volumes...\n",
      "done creating validation dataset\n",
      "epoch 1 | loss: 0.8578403658018663\n",
      "epoch 1 | val_loss: 0.9384000528426397\n",
      "epoch 1 | val_dsc: 0.32827017005499615\n",
      "epoch 2 | loss: 0.8275112446684104\n",
      "epoch 2 | val_loss: 0.9292328556378683\n",
      "epoch 2 | val_dsc: 0.6145270874275711\n",
      "epoch 3 | loss: 0.799904200893182\n",
      "epoch 3 | val_loss: 0.9188103931290763\n",
      "epoch 3 | val_dsc: 0.3357654501921691\n",
      "epoch 4 | loss: 0.7630986812023016\n",
      "epoch 4 | val_loss: 0.8995079540071034\n",
      "epoch 4 | val_dsc: 0.6540705989632051\n",
      "epoch 5 | loss: 0.7142191039255033\n",
      "epoch 5 | val_loss: 0.8748797745931716\n",
      "epoch 5 | val_dsc: 0.7143958051936092\n",
      "epoch 6 | loss: 0.6500823850241991\n",
      "epoch 6 | val_loss: 0.8452343798819042\n",
      "epoch 6 | val_dsc: 0.6108247538611097\n",
      "epoch 7 | loss: 0.5736705609239064\n",
      "epoch 7 | val_loss: 0.8000747731753758\n",
      "epoch 7 | val_dsc: 0.7791792259602571\n",
      "epoch 8 | loss: 0.4917515324285397\n",
      "epoch 8 | val_loss: 0.7620977475529626\n",
      "epoch 8 | val_dsc: 0.7322950619510613\n",
      "epoch 9 | loss: 0.4076263148051042\n",
      "epoch 9 | val_loss: 0.7041210134824117\n",
      "epoch 9 | val_dsc: 0.7666114191574913\n",
      "epoch 10 | loss: 0.3295874142876038\n",
      "epoch 10 | val_loss: 0.6478918734050932\n",
      "epoch 10 | val_dsc: 0.7982770338124194\n",
      "epoch 11 | loss: 0.2707200970214147\n",
      "epoch 11 | val_loss: 0.5959200944219317\n",
      "epoch 11 | val_dsc: 0.8453917027999063\n",
      "epoch 12 | loss: 0.2218643595966009\n",
      "epoch 12 | val_loss: 0.5547481576601664\n",
      "epoch 12 | val_dsc: 0.8420152742511391\n",
      "epoch 13 | loss: 0.1880242033646657\n",
      "epoch 13 | val_loss: 0.5375010967254639\n",
      "epoch 13 | val_dsc: 0.785972010005531\n",
      "epoch 14 | loss: 0.16230937867210463\n",
      "epoch 14 | val_loss: 0.495731668812888\n",
      "epoch 14 | val_dsc: 0.8485041269640717\n",
      "epoch 15 | loss: 0.14232384499448997\n",
      "epoch 15 | val_loss: 0.4705884967531477\n",
      "epoch 15 | val_dsc: 0.8650956646883394\n",
      "epoch 16 | loss: 0.12610927539376113\n",
      "epoch 16 | val_loss: 0.4559173782666524\n",
      "epoch 16 | val_dsc: 0.8488135500527667\n",
      "epoch 17 | loss: 0.11347547517373012\n",
      "epoch 17 | val_loss: 0.4437597706204369\n",
      "epoch 17 | val_dsc: 0.8394459594129577\n",
      "epoch 18 | loss: 0.10438783982625374\n",
      "epoch 18 | val_loss: 0.42976631153197514\n",
      "epoch 18 | val_dsc: 0.8560308482001634\n",
      "epoch 19 | loss: 0.10166960582137108\n",
      "epoch 19 | val_loss: 0.4061539655640012\n",
      "epoch 19 | val_dsc: 0.859078051038478\n",
      "epoch 20 | loss: 0.09048767445179132\n",
      "epoch 20 | val_loss: 0.4034594978604998\n",
      "epoch 20 | val_dsc: 0.8439061225428441\n",
      "epoch 21 | loss: 0.08409606779997165\n",
      "epoch 21 | val_loss: 0.3934345188595\n",
      "epoch 21 | val_dsc: 0.8463763060667221\n",
      "epoch 22 | loss: 0.07898417057899329\n",
      "epoch 22 | val_loss: 0.3787716258139837\n",
      "epoch 22 | val_dsc: 0.8599513358164741\n",
      "epoch 23 | loss: 0.07502928605446449\n",
      "epoch 23 | val_loss: 0.370536364260174\n",
      "epoch 23 | val_dsc: 0.8648473359053412\n",
      "epoch 24 | loss: 0.07246920695671669\n",
      "epoch 24 | val_loss: 0.38245396387009395\n",
      "epoch 24 | val_dsc: 0.8397895781984694\n",
      "epoch 25 | loss: 0.06887296845133488\n",
      "epoch 25 | val_loss: 0.3651834499268305\n",
      "epoch 25 | val_dsc: 0.857721976474274\n",
      "epoch 26 | loss: 0.06768055030932793\n",
      "epoch 26 | val_loss: 0.35854523522513254\n",
      "epoch 26 | val_dsc: 0.8616593662597607\n",
      "epoch 27 | loss: 0.06351356781446017\n",
      "epoch 27 | val_loss: 0.3414793468656994\n",
      "epoch 27 | val_dsc: 0.8736391463957481\n",
      "epoch 28 | loss: 0.0612757930961939\n",
      "epoch 28 | val_loss: 0.3503942546390352\n",
      "epoch 28 | val_dsc: 0.8537350430559831\n",
      "epoch 29 | loss: 0.058244482829020575\n",
      "epoch 29 | val_loss: 0.34752107518059866\n",
      "epoch 29 | val_dsc: 0.8607275149582444\n",
      "epoch 30 | loss: 0.056295900677259154\n",
      "epoch 30 | val_loss: 0.32787941751025973\n",
      "epoch 30 | val_dsc: 0.8837672453777052\n",
      "epoch 31 | loss: 0.05516511287826758\n",
      "epoch 31 | val_loss: 0.3465682920955476\n",
      "epoch 31 | val_dsc: 0.8480700175047383\n",
      "epoch 32 | loss: 0.05338291222086319\n",
      "epoch 32 | val_loss: 0.3358026459103539\n",
      "epoch 32 | val_dsc: 0.8663367908059287\n",
      "epoch 33 | loss: 0.052886347071482584\n",
      "epoch 33 | val_loss: 0.34632458289464313\n",
      "epoch 33 | val_dsc: 0.8579368503307618\n",
      "epoch 34 | loss: 0.0527587398313559\n",
      "epoch 34 | val_loss: 0.3247250829424177\n",
      "epoch 34 | val_dsc: 0.8900012591918587\n",
      "epoch 35 | loss: 0.05094864305395346\n",
      "epoch 35 | val_loss: 0.32404076769238427\n",
      "epoch 35 | val_dsc: 0.8841845189487063\n",
      "epoch 36 | loss: 0.049143653362989426\n",
      "epoch 36 | val_loss: 0.3439455089114961\n",
      "epoch 36 | val_dsc: 0.8347257041138232\n",
      "epoch 37 | loss: 0.04871563298197893\n",
      "epoch 37 | val_loss: 0.3304645816485087\n",
      "epoch 37 | val_dsc: 0.8583610582691354\n",
      "epoch 38 | loss: 0.04875301311795528\n",
      "epoch 38 | val_loss: 0.30871013800303143\n",
      "epoch 38 | val_dsc: 0.8688603711824697\n",
      "epoch 39 | loss: 0.04742038909059305\n",
      "epoch 39 | val_loss: 0.3152652581532796\n",
      "epoch 39 | val_dsc: 0.8765481632497215\n",
      "epoch 40 | loss: 0.04560279416350218\n",
      "epoch 40 | val_loss: 0.33804878450575326\n",
      "epoch 40 | val_dsc: 0.8307516888415094\n",
      "epoch 41 | loss: 0.046966672803346925\n",
      "epoch 41 | val_loss: 0.3204580318360102\n",
      "epoch 41 | val_dsc: 0.8677102585613998\n",
      "epoch 42 | loss: 0.04447125041714081\n",
      "epoch 42 | val_loss: 0.32408574649265837\n",
      "epoch 42 | val_dsc: 0.8658522567961711\n",
      "epoch 43 | loss: 0.044902190852623716\n",
      "epoch 43 | val_loss: 0.32385188341140747\n",
      "epoch 43 | val_dsc: 0.8759218173384598\n",
      "epoch 44 | loss: 0.043476490733715206\n",
      "epoch 44 | val_loss: 0.3158870424543108\n",
      "epoch 44 | val_dsc: 0.8698726237818665\n",
      "epoch 45 | loss: 0.04203078809839029\n",
      "epoch 45 | val_loss: 0.31667122102919076\n",
      "epoch 45 | val_dsc: 0.8790200823151653\n",
      "epoch 46 | loss: 0.04087855007786017\n",
      "epoch 46 | val_loss: 0.316200057665507\n",
      "epoch 46 | val_dsc: 0.8829228024547884\n",
      "epoch 47 | loss: 0.040489897705041446\n",
      "epoch 47 | val_loss: 0.31357427154268536\n",
      "epoch 47 | val_dsc: 0.8768820080118035\n",
      "epoch 48 | loss: 0.04025604719152817\n",
      "epoch 48 | val_loss: 0.3091037159874326\n",
      "epoch 48 | val_dsc: 0.8738408752081916\n",
      "epoch 49 | loss: 0.04014307919603128\n",
      "epoch 49 | val_loss: 0.3116202723412287\n",
      "epoch 49 | val_dsc: 0.8714563186267827\n",
      "epoch 50 | loss: 0.039635260231219806\n",
      "epoch 50 | val_loss: 0.32227035363515216\n",
      "epoch 50 | val_dsc: 0.87287369887355\n",
      "\n",
      "Best validation mean DSC: 0.890001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-1bc1678395cd>:564: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
      "<ipython-input-3-1bc1678395cd>:700: UserWarning: ./TCGA_DU_7014_19860618-57.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-3-1bc1678395cd>:700: UserWarning: ./TCGA_DU_6408_19860521-51.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-3-1bc1678395cd>:700: UserWarning: ./TCGA_DU_6408_19860521-52.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-3-1bc1678395cd>:700: UserWarning: ./TCGA_DU_6408_19860521-53.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-3-1bc1678395cd>:700: UserWarning: ./TCGA_DU_6404_19850629-50.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-3-1bc1678395cd>:700: UserWarning: ./TCGA_DU_5851_19950428-34.png is a low contrast image\n",
      "  imsave(filepath, image)\n"
     ]
    }
   ],
   "source": [
    "def crop_sample(x):\n",
    "    volume, mask = x\n",
    "    volume[volume < np.max(volume) * 0.1] = 0\n",
    "    z_projection = np.max(np.max(np.max(volume, axis=-1), axis=-1), axis=-1)\n",
    "    z_nonzero = np.nonzero(z_projection)\n",
    "    z_min = np.min(z_nonzero)\n",
    "    z_max = np.max(z_nonzero) + 1\n",
    "    y_projection = np.max(np.max(np.max(volume, axis=0), axis=-1), axis=-1)\n",
    "    y_nonzero = np.nonzero(y_projection)\n",
    "    y_min = np.min(y_nonzero)\n",
    "    y_max = np.max(y_nonzero) + 1\n",
    "    x_projection = np.max(np.max(np.max(volume, axis=0), axis=0), axis=-1)\n",
    "    x_nonzero = np.nonzero(x_projection)\n",
    "    x_min = np.min(x_nonzero)\n",
    "    x_max = np.max(x_nonzero) + 1\n",
    "    return (\n",
    "        volume[z_min:z_max, y_min:y_max, x_min:x_max],\n",
    "        mask[z_min:z_max, y_min:y_max, x_min:x_max],\n",
    "    )\n",
    "\n",
    "\n",
    "def pad_sample(x):\n",
    "    volume, mask = x\n",
    "    a = volume.shape[1]\n",
    "    b = volume.shape[2]\n",
    "    if a == b:\n",
    "        return volume, mask\n",
    "    diff = (max(a, b) - min(a, b)) / 2.0\n",
    "    if a > b:\n",
    "        padding = ((0, 0), (0, 0), (int(np.floor(diff)), int(np.ceil(diff))))\n",
    "    else:\n",
    "        padding = ((0, 0), (int(np.floor(diff)), int(np.ceil(diff))), (0, 0))\n",
    "    mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
    "    padding = padding + ((0, 0),)\n",
    "    volume = np.pad(volume, padding, mode=\"constant\", constant_values=0)\n",
    "    return volume, mask\n",
    "\n",
    "\n",
    "def resize_sample(x, size=256):\n",
    "    volume, mask = x\n",
    "    v_shape = volume.shape\n",
    "    out_shape = (v_shape[0], size, size)\n",
    "    mask = resize(\n",
    "        mask,\n",
    "        output_shape=out_shape,\n",
    "        order=0,\n",
    "        mode=\"constant\",\n",
    "        cval=0,\n",
    "        anti_aliasing=False,\n",
    "    )\n",
    "    out_shape = out_shape + (v_shape[3],)\n",
    "    volume = resize(\n",
    "        volume,\n",
    "        output_shape=out_shape,\n",
    "        order=2,\n",
    "        mode=\"constant\",\n",
    "        cval=0,\n",
    "        anti_aliasing=False,\n",
    "    )\n",
    "    return volume, mask\n",
    "\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    p10 = np.percentile(volume, 10)\n",
    "    p99 = np.percentile(volume, 99)\n",
    "    volume = rescale_intensity(volume, in_range=(p10, p99))\n",
    "    m = np.mean(volume, axis=(0, 1, 2))\n",
    "    s = np.std(volume, axis=(0, 1, 2))\n",
    "    volume = (volume - m) / s\n",
    "    return volume\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
    "\n",
    "    in_channels = 3\n",
    "    out_channels = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        transform=None,\n",
    "        image_size=256,\n",
    "        subset=\"train\",\n",
    "        random_sampling=True,\n",
    "        seed=42,\n",
    "    ):\n",
    "        assert subset in [\"all\", \"train\", \"validation\"]\n",
    "\n",
    "        # read images\n",
    "        volumes = {}\n",
    "        masks = {}\n",
    "        print(\"reading {} images...\".format(subset))\n",
    "        for (dirpath, dirnames, filenames) in os.walk(images_dir):\n",
    "            image_slices = []\n",
    "            mask_slices = []\n",
    "            for filename in sorted(\n",
    "                filter(lambda f: \".tif\" in f, filenames),\n",
    "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
    "            ):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                if \"mask\" in filename:\n",
    "                    mask_slices.append(imread(filepath, as_gray=True))\n",
    "                else:\n",
    "                    image_slices.append(imread(filepath))\n",
    "            if len(image_slices) > 0:\n",
    "                patient_id = dirpath.split(\"/\")[-1]\n",
    "                volumes[patient_id] = np.array(image_slices[1:-1])\n",
    "                masks[patient_id] = np.array(mask_slices[1:-1])\n",
    "\n",
    "        self.patients = sorted(volumes)\n",
    "\n",
    "        # select cases to subset\n",
    "        if not subset == \"all\":\n",
    "            random.seed(seed)\n",
    "            validation_patients = random.sample(self.patients, k=10)\n",
    "            if subset == \"validation\":\n",
    "                self.patients = validation_patients\n",
    "            else:\n",
    "                self.patients = sorted(\n",
    "                    list(set(self.patients).difference(validation_patients))\n",
    "                )\n",
    "\n",
    "        print(\"preprocessing {} volumes...\".format(subset))\n",
    "        # create list of tuples (volume, mask)\n",
    "        self.volumes = [(volumes[k], masks[k]) for k in self.patients]\n",
    "\n",
    "        print(\"cropping {} volumes...\".format(subset))\n",
    "        # crop to smallest enclosing volume\n",
    "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
    "\n",
    "        print(\"padding {} volumes...\".format(subset))\n",
    "        # pad to square\n",
    "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
    "\n",
    "        print(\"resizing {} volumes...\".format(subset))\n",
    "        # resize\n",
    "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
    "\n",
    "        print(\"normalizing {} volumes...\".format(subset))\n",
    "        # normalize channel-wise\n",
    "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]\n",
    "\n",
    "        # probabilities for sampling slices based on masks\n",
    "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes]\n",
    "        self.slice_weights = [\n",
    "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights\n",
    "        ]\n",
    "\n",
    "        # add channel dimension to masks\n",
    "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes]\n",
    "\n",
    "        print(\"done creating {} dataset\".format(subset))\n",
    "\n",
    "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
    "        num_slices = [v.shape[0] for v, m in self.volumes]\n",
    "        self.patient_slice_index = list(\n",
    "            zip(\n",
    "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []),\n",
    "                sum([list(range(x)) for x in num_slices], []),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.random_sampling = random_sampling\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_slice_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient = self.patient_slice_index[idx][0]\n",
    "        slice_n = self.patient_slice_index[idx][1]\n",
    "\n",
    "        if self.random_sampling:\n",
    "            patient = np.random.randint(len(self.volumes))\n",
    "            slice_n = np.random.choice(\n",
    "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
    "            )\n",
    "\n",
    "        v, m = self.volumes[patient]\n",
    "        image = v[slice_n]\n",
    "        mask = m[slice_n]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform((image, mask))\n",
    "\n",
    "        # fix dimensions (C, H, W)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        mask = mask.transpose(2, 0, 1)\n",
    "\n",
    "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
    "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
    "\n",
    "        # return tensors\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "def transforms(scale=None, angle=None, flip_prob=None):\n",
    "    transform_list = []\n",
    "\n",
    "    if scale is not None:\n",
    "        transform_list.append(Scale(scale))\n",
    "    if angle is not None:\n",
    "        transform_list.append(Rotate(angle))\n",
    "    if flip_prob is not None:\n",
    "        transform_list.append(HorizontalFlip(flip_prob))\n",
    "\n",
    "    return Compose(transform_list)\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "\n",
    "    def __init__(self, scale):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample\n",
    "\n",
    "        img_size = image.shape[0]\n",
    "\n",
    "        scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n",
    "\n",
    "        image = rescale(\n",
    "            image,\n",
    "            (scale, scale),\n",
    "            multichannel=True,\n",
    "            preserve_range=True,\n",
    "            mode=\"constant\",\n",
    "            anti_aliasing=False,\n",
    "        )\n",
    "        mask = rescale(\n",
    "            mask,\n",
    "            (scale, scale),\n",
    "            order=0,\n",
    "            multichannel=True,\n",
    "            preserve_range=True,\n",
    "            mode=\"constant\",\n",
    "            anti_aliasing=False,\n",
    "        )\n",
    "\n",
    "        if scale < 1.0:\n",
    "            diff = (img_size - image.shape[0]) / 2.0\n",
    "            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n",
    "            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n",
    "            mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
    "        else:\n",
    "            x_min = (image.shape[0] - img_size) // 2\n",
    "            x_max = x_min + img_size\n",
    "            image = image[x_min:x_max, x_min:x_max, ...]\n",
    "            mask = mask[x_min:x_max, x_min:x_max, ...]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class Rotate(object):\n",
    "\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample\n",
    "\n",
    "        angle = np.random.uniform(low=-self.angle, high=self.angle)\n",
    "        image = rotate(image, angle, resize=False, preserve_range=True, mode=\"constant\")\n",
    "        mask = rotate(\n",
    "            mask, angle, resize=False, order=0, preserve_range=True, mode=\"constant\"\n",
    "        )\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class HorizontalFlip(object):\n",
    "\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample\n",
    "\n",
    "        if np.random.rand() > self.flip_prob:\n",
    "            return image, mask\n",
    "\n",
    "        image = np.fliplr(image).copy()\n",
    "        mask = np.fliplr(mask).copy()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "        features = init_features\n",
    "        \n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = 1.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "            y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc\n",
    "\n",
    "\n",
    "def log_images(x, y_true, y_pred, channel=1):\n",
    "    images = []\n",
    "    x_np = x[:, channel].cpu().numpy()\n",
    "    y_true_np = y_true[:, 0].cpu().numpy()\n",
    "    y_pred_np = y_pred[:, 0].cpu().numpy()\n",
    "    for i in range(x_np.shape[0]):\n",
    "        image = gray2rgb(np.squeeze(x_np[i]))\n",
    "        image = outline(image, y_pred_np[i], color=[255, 0, 0])\n",
    "        image = outline(image, y_true_np[i], color=[0, 255, 0])\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def gray2rgb(image):\n",
    "    w, h = image.shape\n",
    "    image += np.abs(np.min(image))\n",
    "    image_max = np.abs(np.max(image))\n",
    "    if image_max > 0:\n",
    "        image /= image_max\n",
    "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
    "    ret[:, :, 2] = ret[:, :, 1] = ret[:, :, 0] = image * 255\n",
    "    return ret\n",
    "\n",
    "\n",
    "def outline(image, mask, color):\n",
    "    mask = np.round(mask)\n",
    "    yy, xx = np.nonzero(mask)\n",
    "    for y, x in zip(yy, xx):\n",
    "        if 0.0 < np.mean(mask[max(0, y - 1) : y + 2, max(0, x - 1) : x + 2]) < 1.0:\n",
    "            image[max(0, y) : y + 1, max(0, x) : x + 1] = color\n",
    "    return image\n",
    "\n",
    "\n",
    "def data_loaders(batch_size, workers, image_size, aug_scale, aug_angle):\n",
    "    dataset_train, dataset_valid = datasets(\"../dataset/lgg-mri-segmentation/kaggle_3m\", image_size, aug_scale, aug_angle)\n",
    "\n",
    "    def worker_init(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        worker_init_fn=worker_init,\n",
    "    )\n",
    "    loader_valid = DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        worker_init_fn=worker_init,\n",
    "    )\n",
    "\n",
    "    return loader_train, loader_valid\n",
    "\n",
    "\n",
    "def datasets(images, image_size, aug_scale, aug_angle):\n",
    "    train = BrainSegmentationDataset(\n",
    "        images_dir=images,\n",
    "        subset=\"train\",\n",
    "        image_size=image_size,\n",
    "        transform=transforms(scale=aug_scale, angle=aug_angle, flip_prob=0.5),\n",
    "    )\n",
    "    valid = BrainSegmentationDataset(\n",
    "        images_dir=images,\n",
    "        subset=\"validation\",\n",
    "        image_size=image_size,\n",
    "        random_sampling=False,\n",
    "    )\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def dsc(y_pred, y_true):\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    y_true = np.round(y_true).astype(int)\n",
    "    return np.sum(y_pred[y_true == 1]) * 2.0 / (np.sum(y_pred) + np.sum(y_true))\n",
    "\n",
    "\n",
    "def dsc_distribution(volumes):\n",
    "    dsc_dict = {}\n",
    "    for p in volumes:\n",
    "        y_pred = volumes[p][1]\n",
    "        y_true = volumes[p][2]\n",
    "        dsc_dict[p] = dsc(y_pred, y_true)\n",
    "    return dsc_dict\n",
    "\n",
    "\n",
    "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
    "    dsc_list = []\n",
    "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
    "    index = 0\n",
    "    for p in range(len(num_slices)):\n",
    "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
    "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
    "        dsc_list.append(dsc(y_pred, y_true))\n",
    "        index += num_slices[p]\n",
    "    return dsc_list\n",
    "\n",
    "\n",
    "def postprocess_per_volume(\n",
    "    input_list, pred_list, true_list, patient_slice_index, patients\n",
    "):\n",
    "    volumes = {}\n",
    "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
    "    index = 0\n",
    "    for p in range(len(num_slices)):\n",
    "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
    "        volume_pred = np.round(\n",
    "            np.array(pred_list[index : index + num_slices[p]])\n",
    "        ).astype(int)\n",
    "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
    "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
    "        index += num_slices[p]\n",
    "    return volumes\n",
    "\n",
    "\n",
    "def log_loss_summary(loss, step, prefix=\"\"):\n",
    "    print(\"epoch {} | {}: {}\".format(step + 1, prefix + \"loss\", np.mean(loss)))\n",
    "\n",
    "def log_scalar_summary(tag, value, step):\n",
    "    print(\"epoch {} | {}: {}\".format(step + 1, tag, value))\n",
    "\n",
    "\n",
    "def plot_dsc(dsc_dist):\n",
    "    y_positions = np.arange(len(dsc_dist))\n",
    "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
    "    values = [x[1] for x in dsc_dist]\n",
    "    labels = [x[0] for x in dsc_dist]\n",
    "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
    "    plt.yticks(y_positions, labels)\n",
    "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
    "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
    "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
    "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
    "    plt.tight_layout()\n",
    "    canvas.draw()\n",
    "    plt.close()\n",
    "    s, (width, height) = canvas.print_to_buffer()\n",
    "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "workers = 2\n",
    "weights = \"./\"\n",
    "image_size = 224\n",
    "aug_scale = 0.05\n",
    "aug_angle = 15\n",
    "\n",
    "\n",
    "def train_validate():\n",
    "    device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
    "    \n",
    "    loader_train, loader_valid = data_loaders(batch_size, workers, image_size, aug_scale, aug_angle)\n",
    "    loaders = {\"train\": loader_train, \"valid\": loader_valid}\n",
    "    \n",
    "    unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
    "    unet.to(device)\n",
    "    \n",
    "    dsc_loss = DiceLoss()\n",
    "    best_validation_dsc = 0.0\n",
    "    \n",
    "    optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
    "    \n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                unet.train()\n",
    "            else:\n",
    "                unet.eval()\n",
    "    \n",
    "            validation_pred = []\n",
    "            validation_true = []\n",
    "    \n",
    "            for i, data in enumerate(loaders[phase]):\n",
    "                if phase == \"train\":\n",
    "                    step += 1\n",
    "    \n",
    "                x, y_true = data\n",
    "                x, y_true = x.to(device), y_true.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    y_pred = unet(x)\n",
    "    \n",
    "                    loss = dsc_loss(y_pred, y_true)\n",
    "    \n",
    "                    if phase == \"valid\":\n",
    "                        loss_valid.append(loss.item())\n",
    "                        y_pred_np = y_pred.detach().cpu().numpy()\n",
    "                        validation_pred.extend(\n",
    "                            [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
    "                        )\n",
    "                        y_true_np = y_true.detach().cpu().numpy()\n",
    "                        validation_true.extend(\n",
    "                            [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
    "                        )\n",
    "                        \n",
    "                    if phase == \"train\":\n",
    "                        loss_train.append(loss.item())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "    \n",
    "            if phase == \"train\":\n",
    "                log_loss_summary(loss_train, epoch)\n",
    "                loss_train = []\n",
    "\n",
    "            if phase == \"valid\":\n",
    "                log_loss_summary(loss_valid, epoch, prefix=\"val_\")\n",
    "                mean_dsc = np.mean(\n",
    "                    dsc_per_volume(\n",
    "                        validation_pred,\n",
    "                        validation_true,\n",
    "                        loader_valid.dataset.patient_slice_index,\n",
    "                    )\n",
    "                )\n",
    "                log_scalar_summary(\"val_dsc\", mean_dsc, epoch)\n",
    "                if mean_dsc > best_validation_dsc:\n",
    "                    best_validation_dsc = mean_dsc\n",
    "                    torch.save(unet.state_dict(), os.path.join(weights, \"unet.pt\"))\n",
    "                loss_valid = []\n",
    "    \n",
    "    print(\"\\nBest validation mean DSC: {:4f}\\n\".format(best_validation_dsc))\n",
    "    \n",
    "    state_dict = torch.load(os.path.join(weights, \"unet.pt\"))\n",
    "    unet.load_state_dict(state_dict)\n",
    "    unet.eval()\n",
    "    \n",
    "    input_list = []\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    \n",
    "    for i, data in enumerate(loader_valid):\n",
    "        x, y_true = data\n",
    "        x, y_true = x.to(device), y_true.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = unet(x)\n",
    "            y_pred_np = y_pred.detach().cpu().numpy()\n",
    "            pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
    "            y_true_np = y_true.detach().cpu().numpy()\n",
    "            true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
    "            x_np = x.detach().cpu().numpy()\n",
    "            input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
    "            \n",
    "    volumes = postprocess_per_volume(\n",
    "        input_list,\n",
    "        pred_list,\n",
    "        true_list,\n",
    "        loader_valid.dataset.patient_slice_index,\n",
    "        loader_valid.dataset.patients,\n",
    "    )\n",
    "    \n",
    "    dsc_dist = dsc_distribution(volumes)\n",
    "\n",
    "    dsc_dist_plot = plot_dsc(dsc_dist)\n",
    "    imsave(\"./dsc.png\", dsc_dist_plot)\n",
    "\n",
    "    for p in volumes:\n",
    "        x = volumes[p][0]\n",
    "        y_pred = volumes[p][1]\n",
    "        y_true = volumes[p][2]\n",
    "        for s in range(x.shape[0]):\n",
    "            image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
    "            image = outline(image, y_pred[s, 0], color=[255, 0, 0])\n",
    "            image = outline(image, y_true[s, 0], color=[0, 255, 0])\n",
    "            filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
    "            filepath = os.path.join(\"./\", filename)\n",
    "            imsave(filepath, image)\n",
    "\n",
    "\n",
    "train_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-intention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-moisture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-newcastle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-aquarium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-cleaner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-announcement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-quest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
